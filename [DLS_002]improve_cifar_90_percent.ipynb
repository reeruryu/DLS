{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "84e115a8",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-03-14T01:09:01.717666Z",
          "iopub.status.busy": "2023-03-14T01:09:01.716836Z",
          "iopub.status.idle": "2023-03-14T01:09:01.728895Z",
          "shell.execute_reply": "2023-03-14T01:09:01.728020Z"
        },
        "papermill": {
          "duration": 0.02092,
          "end_time": "2023-03-14T01:09:01.731266",
          "exception": false,
          "start_time": "2023-03-14T01:09:01.710346",
          "status": "completed"
        },
        "tags": [],
        "id": "84e115a8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "73af4a65",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-14T01:09:01.739601Z",
          "iopub.status.busy": "2023-03-14T01:09:01.739328Z",
          "iopub.status.idle": "2023-03-14T01:09:04.014767Z",
          "shell.execute_reply": "2023-03-14T01:09:04.013707Z"
        },
        "papermill": {
          "duration": 2.282152,
          "end_time": "2023-03-14T01:09:04.017231",
          "exception": false,
          "start_time": "2023-03-14T01:09:01.735079",
          "status": "completed"
        },
        "tags": [],
        "id": "73af4a65"
      },
      "outputs": [],
      "source": [
        "#random seed 고정\n",
        "import random\n",
        "import torch\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available() : \n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8fadca99",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-14T01:09:04.034495Z",
          "iopub.status.busy": "2023-03-14T01:09:04.034044Z",
          "iopub.status.idle": "2023-03-14T01:09:14.268518Z",
          "shell.execute_reply": "2023-03-14T01:09:14.267457Z"
        },
        "papermill": {
          "duration": 10.241189,
          "end_time": "2023-03-14T01:09:14.270834",
          "exception": false,
          "start_time": "2023-03-14T01:09:04.029645",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fadca99",
        "outputId": "fe6ea981-7ba7-4801-8a5c-3dbb15d4c6c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "training data =  (32, 32, 3)\n",
            "testing data =  (32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "# CIFAR10 데이터 셋 불러오기\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 배치사이즈 설정\n",
        "batch_size = 64\n",
        "\n",
        "# CIFAR10의 Train 데이터의 평균과 표준편차값을 구하기 위한 과정\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', download=True, train=True) \n",
        "mean = dataset.data.mean(axis=(0,1,2))\n",
        "std = dataset.data.std(axis=(0,1,2))\n",
        "mean = mean / 255 # 평균\n",
        "std = std / 255 # 표준편차\n",
        "\n",
        "\n",
        "# data augmentation 정의\n",
        "# pytorch에서는 torchvision.transforms.ToTensor를 이용하여 [0, 1]까지의 정규화와 torch.FloatTensor형으로 형변환을 함께 진행할 수 있으며,\n",
        "# torchvision.transforms.Normalize를 통해 평균과 표준편차 값을 이용한 정규화를 수행할 수 있음.\n",
        "transform_1 = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1,0.1)),\n",
        "    transforms.RandomHorizontalFlip()\n",
        "    ])\n",
        "transform_2 = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)])\n",
        "\n",
        "# normalize transformation 적용\n",
        "# train 데이터 읽어오기\n",
        "trainSet = torchvision.datasets.CIFAR10(root='./data', \n",
        "                                        train=True, \n",
        "                                        download=True, \n",
        "                                        transform=transform_1)\n",
        "\n",
        "# test 데이터 읽어오기\n",
        "testSet = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                      train=False,\n",
        "                                      download=True,\n",
        "                                      transform=transform_2)\n",
        "\n",
        "print(\"training data = \", trainSet.data[0].shape)\n",
        "print(\"testing data = \", testSet.data[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3ec2c584",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-14T01:09:14.281239Z",
          "iopub.status.busy": "2023-03-14T01:09:14.279764Z",
          "iopub.status.idle": "2023-03-14T01:09:14.293093Z",
          "shell.execute_reply": "2023-03-14T01:09:14.292168Z"
        },
        "papermill": {
          "duration": 0.020414,
          "end_time": "2023-03-14T01:09:14.295264",
          "exception": false,
          "start_time": "2023-03-14T01:09:14.274850",
          "status": "completed"
        },
        "tags": [],
        "id": "3ec2c584"
      },
      "outputs": [],
      "source": [
        "# model 정의\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module) :\n",
        "    def __init__(self) :\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(3,32,kernel_size=3,padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32)\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(32,32,kernel_size=3,padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        self.conv_block_3 = nn.Sequential(\n",
        "            nn.Conv2d(32,64,kernel_size=3,padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "        self.conv_block_4 = nn.Sequential(\n",
        "            nn.Conv2d(64,64,kernel_size=3,padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.conv_block_5 = nn.Sequential(\n",
        "            nn.Conv2d(64,128,kernel_size=3,padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128)\n",
        "        )\n",
        "        self.conv_block_6 = nn.Sequential(\n",
        "            nn.Conv2d(128,128,kernel_size=3,padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "        self.dense = nn.Linear(128*4*4,10)\n",
        "        \n",
        "    def forward(self, data) :\n",
        "        x = self.conv_block_1(data)\n",
        "        x = self.conv_block_2(x)\n",
        "        x = self.conv_block_3(x)\n",
        "        x = self.conv_block_4(x)\n",
        "        x = self.conv_block_5(x)\n",
        "        x = self.conv_block_6(x)\n",
        "        \n",
        "        x = x.view(-1, 128*4*4)\n",
        "        x = self.dense(x)\n",
        "        output = F.log_softmax(x,dim=1)\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "606c492e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-14T01:09:14.304150Z",
          "iopub.status.busy": "2023-03-14T01:09:14.303851Z",
          "iopub.status.idle": "2023-03-14T01:09:14.311086Z",
          "shell.execute_reply": "2023-03-14T01:09:14.310169Z"
        },
        "papermill": {
          "duration": 0.014002,
          "end_time": "2023-03-14T01:09:14.313093",
          "exception": false,
          "start_time": "2023-03-14T01:09:14.299091",
          "status": "completed"
        },
        "tags": [],
        "id": "606c492e",
        "outputId": "7dc4f283-8db0-411d-90d9-eb02cc6c2269",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE:cuda\n"
          ]
        }
      ],
      "source": [
        "# DEVICE 초기화\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"DEVICE:{DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e97a9302",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-14T01:09:14.321902Z",
          "iopub.status.busy": "2023-03-14T01:09:14.321623Z",
          "iopub.status.idle": "2023-03-14T01:09:16.997697Z",
          "shell.execute_reply": "2023-03-14T01:09:16.996643Z"
        },
        "papermill": {
          "duration": 2.683264,
          "end_time": "2023-03-14T01:09:17.000251",
          "exception": false,
          "start_time": "2023-03-14T01:09:14.316987",
          "status": "completed"
        },
        "tags": [],
        "id": "e97a9302"
      },
      "outputs": [],
      "source": [
        "# model 초기화\n",
        "model = Net().to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ec114a88",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-14T01:09:17.011091Z",
          "iopub.status.busy": "2023-03-14T01:09:17.009592Z",
          "iopub.status.idle": "2023-03-14T01:09:27.278936Z",
          "shell.execute_reply": "2023-03-14T01:09:27.277742Z"
        },
        "papermill": {
          "duration": 10.277126,
          "end_time": "2023-03-14T01:09:27.281611",
          "exception": false,
          "start_time": "2023-03-14T01:09:17.004485",
          "status": "completed"
        },
        "tags": [],
        "id": "ec114a88",
        "outputId": "af37f99b-cccf-41c2-bc03-f9b57e699ca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.9/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install torchsummary # model을 요약하기 위해 torchsummary 설치\n",
        "from torchsummary import summary as summary_# 모델 정보를 확인하기 위해 torchsummary 함수 import\n",
        "\n",
        "# 모델의 형태를 출력하기 위한 함수 \n",
        "def summary_model(model,input_shape=(3, 32, 32)):\n",
        "    model = model.cuda()\n",
        "    summary_(model, input_shape) # (model, (input shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f6c3027b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-14T01:09:27.291743Z",
          "iopub.status.busy": "2023-03-14T01:09:27.291406Z",
          "iopub.status.idle": "2023-03-14T01:09:31.801556Z",
          "shell.execute_reply": "2023-03-14T01:09:31.799421Z"
        },
        "papermill": {
          "duration": 4.517823,
          "end_time": "2023-03-14T01:09:31.803861",
          "exception": false,
          "start_time": "2023-03-14T01:09:27.286038",
          "status": "completed"
        },
        "tags": [],
        "id": "f6c3027b",
        "outputId": "a97428bc-1bdd-4af0-f17e-123051ebfde5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "              ReLU-2           [-1, 32, 32, 32]               0\n",
            "       BatchNorm2d-3           [-1, 32, 32, 32]              64\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,248\n",
            "              ReLU-5           [-1, 32, 32, 32]               0\n",
            "       BatchNorm2d-6           [-1, 32, 32, 32]              64\n",
            "         MaxPool2d-7           [-1, 32, 16, 16]               0\n",
            "           Dropout-8           [-1, 32, 16, 16]               0\n",
            "            Conv2d-9           [-1, 64, 16, 16]          18,496\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "      BatchNorm2d-11           [-1, 64, 16, 16]             128\n",
            "           Conv2d-12           [-1, 64, 16, 16]          36,928\n",
            "             ReLU-13           [-1, 64, 16, 16]               0\n",
            "      BatchNorm2d-14           [-1, 64, 16, 16]             128\n",
            "        MaxPool2d-15             [-1, 64, 8, 8]               0\n",
            "          Dropout-16             [-1, 64, 8, 8]               0\n",
            "           Conv2d-17            [-1, 128, 8, 8]          73,856\n",
            "             ReLU-18            [-1, 128, 8, 8]               0\n",
            "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
            "           Conv2d-20            [-1, 128, 8, 8]         147,584\n",
            "             ReLU-21            [-1, 128, 8, 8]               0\n",
            "      BatchNorm2d-22            [-1, 128, 8, 8]             256\n",
            "        MaxPool2d-23            [-1, 128, 4, 4]               0\n",
            "          Dropout-24            [-1, 128, 4, 4]               0\n",
            "           Linear-25                   [-1, 10]          20,490\n",
            "================================================================\n",
            "Total params: 308,394\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.84\n",
            "Params size (MB): 1.18\n",
            "Estimated Total Size (MB): 4.03\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "summary_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1b64ca8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-14T01:09:31.814838Z",
          "iopub.status.busy": "2023-03-14T01:09:31.814267Z",
          "iopub.status.idle": "2023-03-14T07:15:14.748310Z",
          "shell.execute_reply": "2023-03-14T07:15:14.747028Z"
        },
        "papermill": {
          "duration": 21942.95996,
          "end_time": "2023-03-14T07:15:14.768448",
          "exception": false,
          "start_time": "2023-03-14T01:09:31.808488",
          "status": "completed"
        },
        "tags": [],
        "id": "c1b64ca8",
        "outputId": "4b247d88-a854-4538-9271-b7f8f7ed2b58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/250\n",
            "Epoch 00000: val_loss improved from 100.00000 to 1.07881, saving model to bestCheckPoint.pth\n",
            "89s - loss: 1.54333 - acc: 0.44742 - val_loss: 1.07881 - val_acc: 0.61360\n",
            "Epoch 1/250\n",
            "Epoch 00001: val_loss improved from 1.07881 to 0.86217, saving model to bestCheckPoint.pth\n",
            "88s - loss: 1.17371 - acc: 0.58178 - val_loss: 0.86217 - val_acc: 0.69080\n",
            "Epoch 2/250\n",
            "Epoch 00002: val_loss improved from 0.86217 to 0.77518, saving model to bestCheckPoint.pth\n",
            "87s - loss: 1.01583 - acc: 0.64026 - val_loss: 0.77518 - val_acc: 0.73000\n",
            "Epoch 3/250\n",
            "Epoch 00003: val_loss improved from 0.77518 to 0.69588, saving model to bestCheckPoint.pth\n",
            "88s - loss: 0.93066 - acc: 0.67382 - val_loss: 0.69588 - val_acc: 0.75650\n",
            "Epoch 4/250\n",
            "Epoch 00004: val_loss improved from 0.69588 to 0.67219, saving model to bestCheckPoint.pth\n",
            "89s - loss: 0.87001 - acc: 0.69468 - val_loss: 0.67219 - val_acc: 0.75930\n",
            "Epoch 5/250\n",
            "Epoch 00005: val_loss improved from 0.67219 to 0.61652, saving model to bestCheckPoint.pth\n",
            "88s - loss: 0.81911 - acc: 0.71322 - val_loss: 0.61652 - val_acc: 0.78750\n",
            "Epoch 6/250\n",
            "Epoch 00006: val_loss improved from 0.61652 to 0.59915, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.78041 - acc: 0.72784 - val_loss: 0.59915 - val_acc: 0.79510\n",
            "Epoch 7/250\n",
            "Epoch 00007: val_loss improved from 0.59915 to 0.56834, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.75078 - acc: 0.73986 - val_loss: 0.56834 - val_acc: 0.80340\n",
            "Epoch 8/250\n",
            "Epoch 00008: val_loss improved from 0.56834 to 0.53648, saving model to bestCheckPoint.pth\n",
            "88s - loss: 0.71268 - acc: 0.75104 - val_loss: 0.53648 - val_acc: 0.81580\n",
            "Epoch 9/250\n",
            "Epoch 00009: val_loss improved from 0.53648 to 0.51894, saving model to bestCheckPoint.pth\n",
            "89s - loss: 0.69359 - acc: 0.75770 - val_loss: 0.51894 - val_acc: 0.82190\n",
            "Epoch 10/250\n",
            "Epoch 00010: val_loss improved from 0.51894 to 0.50718, saving model to bestCheckPoint.pth\n",
            "88s - loss: 0.66665 - acc: 0.76862 - val_loss: 0.50718 - val_acc: 0.82750\n",
            "Epoch 11/250\n",
            "Epoch 00011: val_loss improved from 0.50718 to 0.50013, saving model to bestCheckPoint.pth\n",
            "88s - loss: 0.64780 - acc: 0.77376 - val_loss: 0.50013 - val_acc: 0.82880\n",
            "Epoch 12/250\n",
            "Epoch 00012: val_loss improved from 0.50013 to 0.49923, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.62995 - acc: 0.78148 - val_loss: 0.49923 - val_acc: 0.82960\n",
            "Epoch 13/250\n",
            "Epoch 00013: val_loss improved from 0.49923 to 0.48869, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.61836 - acc: 0.78548 - val_loss: 0.48869 - val_acc: 0.83670\n",
            "Epoch 14/250\n",
            "Epoch 00014: val_loss improved from 0.48869 to 0.46342, saving model to bestCheckPoint.pth\n",
            "86s - loss: 0.61269 - acc: 0.78576 - val_loss: 0.46342 - val_acc: 0.83980\n",
            "Epoch 15/250\n",
            "Epoch 00015: val_loss improved from 0.46342 to 0.44774, saving model to bestCheckPoint.pth\n",
            "86s - loss: 0.59092 - acc: 0.79436 - val_loss: 0.44774 - val_acc: 0.84490\n",
            "Epoch 16/250\n",
            "Epoch 00016: val_loss improved from 0.44774 to 0.44281, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.57995 - acc: 0.79546 - val_loss: 0.44281 - val_acc: 0.84770\n",
            "Epoch 17/250\n",
            "Epoch 00017: val_loss did not improve\n",
            "88s - loss: 0.56831 - acc: 0.80266 - val_loss: 0.46171 - val_acc: 0.84490\n",
            "Epoch 18/250\n",
            "Epoch 00018: val_loss did not improve\n",
            "87s - loss: 0.55811 - acc: 0.80552 - val_loss: 0.44447 - val_acc: 0.85260\n",
            "Epoch 19/250\n",
            "Epoch 00019: val_loss improved from 0.44281 to 0.42854, saving model to bestCheckPoint.pth\n",
            "88s - loss: 0.54786 - acc: 0.80886 - val_loss: 0.42854 - val_acc: 0.85760\n",
            "Epoch 20/250\n",
            "Epoch 00020: val_loss did not improve\n",
            "88s - loss: 0.54498 - acc: 0.80930 - val_loss: 0.43002 - val_acc: 0.85340\n",
            "Epoch 21/250\n",
            "Epoch 00021: val_loss improved from 0.42854 to 0.42451, saving model to bestCheckPoint.pth\n",
            "88s - loss: 0.53019 - acc: 0.81680 - val_loss: 0.42451 - val_acc: 0.85530\n",
            "Epoch 22/250\n",
            "Epoch 00022: val_loss improved from 0.42451 to 0.42416, saving model to bestCheckPoint.pth\n",
            "88s - loss: 0.52629 - acc: 0.81672 - val_loss: 0.42416 - val_acc: 0.85110\n",
            "Epoch 23/250\n",
            "Epoch 00023: val_loss improved from 0.42416 to 0.41154, saving model to bestCheckPoint.pth\n",
            "88s - loss: 0.51662 - acc: 0.81938 - val_loss: 0.41154 - val_acc: 0.85960\n",
            "Epoch 24/250\n",
            "Epoch 00024: val_loss improved from 0.41154 to 0.40904, saving model to bestCheckPoint.pth\n",
            "88s - loss: 0.51279 - acc: 0.82342 - val_loss: 0.40904 - val_acc: 0.85990\n",
            "Epoch 25/250\n",
            "Epoch 00025: val_loss did not improve\n",
            "88s - loss: 0.50537 - acc: 0.82310 - val_loss: 0.41329 - val_acc: 0.85870\n",
            "Epoch 26/250\n",
            "Epoch 00026: val_loss improved from 0.40904 to 0.39396, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.49813 - acc: 0.82528 - val_loss: 0.39396 - val_acc: 0.86770\n",
            "Epoch 27/250\n",
            "Epoch 00027: val_loss did not improve\n",
            "88s - loss: 0.49156 - acc: 0.82770 - val_loss: 0.39754 - val_acc: 0.86560\n",
            "Epoch 28/250\n",
            "Epoch 00028: val_loss improved from 0.39396 to 0.38469, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.48388 - acc: 0.83074 - val_loss: 0.38469 - val_acc: 0.87100\n",
            "Epoch 29/250\n",
            "Epoch 00029: val_loss improved from 0.38469 to 0.38407, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.48293 - acc: 0.82998 - val_loss: 0.38407 - val_acc: 0.86900\n",
            "Epoch 30/250\n",
            "Epoch 00030: val_loss did not improve\n",
            "87s - loss: 0.47725 - acc: 0.83262 - val_loss: 0.38949 - val_acc: 0.86910\n",
            "Epoch 31/250\n",
            "Epoch 00031: val_loss did not improve\n",
            "87s - loss: 0.47134 - acc: 0.83400 - val_loss: 0.38834 - val_acc: 0.86810\n",
            "Epoch 32/250\n",
            "Epoch 00032: val_loss did not improve\n",
            "88s - loss: 0.46755 - acc: 0.83622 - val_loss: 0.38596 - val_acc: 0.87040\n",
            "Epoch 33/250\n",
            "Epoch 00033: val_loss improved from 0.38407 to 0.36924, saving model to bestCheckPoint.pth\n",
            "89s - loss: 0.46274 - acc: 0.83846 - val_loss: 0.36924 - val_acc: 0.87260\n",
            "Epoch 34/250\n",
            "Epoch 00034: val_loss did not improve\n",
            "88s - loss: 0.45660 - acc: 0.84124 - val_loss: 0.37111 - val_acc: 0.87050\n",
            "Epoch 35/250\n",
            "Epoch 00035: val_loss did not improve\n",
            "89s - loss: 0.45459 - acc: 0.84110 - val_loss: 0.38881 - val_acc: 0.86840\n",
            "Epoch 36/250\n",
            "Epoch 00036: val_loss did not improve\n",
            "88s - loss: 0.45135 - acc: 0.84052 - val_loss: 0.37746 - val_acc: 0.87350\n",
            "Epoch 37/250\n",
            "Epoch 00037: val_loss improved from 0.36924 to 0.35759, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.45014 - acc: 0.84138 - val_loss: 0.35759 - val_acc: 0.87910\n",
            "Epoch 38/250\n",
            "Epoch 00038: val_loss did not improve\n",
            "88s - loss: 0.44545 - acc: 0.84408 - val_loss: 0.37026 - val_acc: 0.87830\n",
            "Epoch 39/250\n",
            "Epoch 00039: val_loss did not improve\n",
            "88s - loss: 0.44172 - acc: 0.84354 - val_loss: 0.36325 - val_acc: 0.87560\n",
            "Epoch 40/250\n",
            "Epoch 00040: val_loss did not improve\n",
            "87s - loss: 0.43664 - acc: 0.84782 - val_loss: 0.36936 - val_acc: 0.87520\n",
            "Epoch 41/250\n",
            "Epoch 00041: val_loss did not improve\n",
            "86s - loss: 0.43183 - acc: 0.84830 - val_loss: 0.37812 - val_acc: 0.87420\n",
            "Epoch 42/250\n",
            "Epoch 00042: val_loss did not improve\n",
            "86s - loss: 0.43170 - acc: 0.84798 - val_loss: 0.36765 - val_acc: 0.87590\n",
            "Epoch 43/250\n",
            "Epoch 00043: val_loss did not improve\n",
            "87s - loss: 0.42951 - acc: 0.85062 - val_loss: 0.36090 - val_acc: 0.87680\n",
            "Epoch 44/250\n",
            "Epoch 00044: val_loss did not improve\n",
            "86s - loss: 0.42146 - acc: 0.85216 - val_loss: 0.37203 - val_acc: 0.87590\n",
            "Epoch 45/250\n",
            "Epoch 00045: val_loss did not improve\n",
            "86s - loss: 0.42221 - acc: 0.85210 - val_loss: 0.36057 - val_acc: 0.87990\n",
            "Epoch 46/250\n",
            "Epoch 00046: val_loss did not improve\n",
            "86s - loss: 0.41854 - acc: 0.85416 - val_loss: 0.36011 - val_acc: 0.88080\n",
            "Epoch 47/250\n",
            "Epoch 00047: val_loss improved from 0.35759 to 0.35308, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.41605 - acc: 0.85446 - val_loss: 0.35308 - val_acc: 0.88130\n",
            "Epoch 48/250\n",
            "Epoch 00048: val_loss did not improve\n",
            "86s - loss: 0.41028 - acc: 0.85486 - val_loss: 0.36238 - val_acc: 0.88030\n",
            "Epoch 49/250\n",
            "Epoch 00049: val_loss improved from 0.35308 to 0.35298, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.41455 - acc: 0.85286 - val_loss: 0.35298 - val_acc: 0.88060\n",
            "Epoch 50/250\n",
            "Epoch 00050: val_loss improved from 0.35298 to 0.34428, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.40577 - acc: 0.85786 - val_loss: 0.34428 - val_acc: 0.88390\n",
            "Epoch 51/250\n",
            "Epoch 00051: val_loss did not improve\n",
            "86s - loss: 0.40740 - acc: 0.85696 - val_loss: 0.34623 - val_acc: 0.88080\n",
            "Epoch 52/250\n",
            "Epoch 00052: val_loss improved from 0.34428 to 0.34274, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.40899 - acc: 0.85738 - val_loss: 0.34274 - val_acc: 0.88120\n",
            "Epoch 53/250\n",
            "Epoch 00053: val_loss did not improve\n",
            "88s - loss: 0.40202 - acc: 0.85850 - val_loss: 0.34358 - val_acc: 0.88490\n",
            "Epoch 54/250\n",
            "Epoch 00054: val_loss did not improve\n",
            "86s - loss: 0.40214 - acc: 0.85876 - val_loss: 0.34291 - val_acc: 0.88030\n",
            "Epoch 55/250\n",
            "Epoch 00055: val_loss did not improve\n",
            "86s - loss: 0.39929 - acc: 0.85956 - val_loss: 0.34675 - val_acc: 0.88220\n",
            "Epoch 56/250\n",
            "Epoch 00056: val_loss did not improve\n",
            "86s - loss: 0.39614 - acc: 0.85928 - val_loss: 0.34357 - val_acc: 0.88470\n",
            "Epoch 57/250\n",
            "Epoch 00057: val_loss improved from 0.34274 to 0.34089, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.39483 - acc: 0.86030 - val_loss: 0.34089 - val_acc: 0.88420\n",
            "Epoch 58/250\n",
            "Epoch 00058: val_loss did not improve\n",
            "87s - loss: 0.39635 - acc: 0.86002 - val_loss: 0.34449 - val_acc: 0.88500\n",
            "Epoch 59/250\n",
            "Epoch 00059: val_loss did not improve\n",
            "86s - loss: 0.38384 - acc: 0.86532 - val_loss: 0.36240 - val_acc: 0.87710\n",
            "Epoch 60/250\n",
            "Epoch 00060: val_loss did not improve\n",
            "87s - loss: 0.38533 - acc: 0.86590 - val_loss: 0.34658 - val_acc: 0.88370\n",
            "Epoch 61/250\n",
            "Epoch 00061: val_loss did not improve\n",
            "86s - loss: 0.38504 - acc: 0.86426 - val_loss: 0.35022 - val_acc: 0.88180\n",
            "Epoch 62/250\n",
            "Epoch 00062: val_loss did not improve\n",
            "87s - loss: 0.38261 - acc: 0.86636 - val_loss: 0.34312 - val_acc: 0.88750\n",
            "Epoch 63/250\n",
            "Epoch 00063: val_loss improved from 0.34089 to 0.33914, saving model to bestCheckPoint.pth\n",
            "86s - loss: 0.38653 - acc: 0.86402 - val_loss: 0.33914 - val_acc: 0.88520\n",
            "Epoch 64/250\n",
            "Epoch 00064: val_loss did not improve\n",
            "88s - loss: 0.38182 - acc: 0.86406 - val_loss: 0.34393 - val_acc: 0.88690\n",
            "Epoch 65/250\n",
            "Epoch 00065: val_loss did not improve\n",
            "87s - loss: 0.37131 - acc: 0.86950 - val_loss: 0.35054 - val_acc: 0.88470\n",
            "Epoch 66/250\n",
            "Epoch 00066: val_loss did not improve\n",
            "86s - loss: 0.37379 - acc: 0.86690 - val_loss: 0.34472 - val_acc: 0.88690\n",
            "Epoch 67/250\n",
            "Epoch 00067: val_loss did not improve\n",
            "86s - loss: 0.37654 - acc: 0.86696 - val_loss: 0.35256 - val_acc: 0.88540\n",
            "Epoch 68/250\n",
            "Epoch 00068: val_loss did not improve\n",
            "86s - loss: 0.37733 - acc: 0.86798 - val_loss: 0.34071 - val_acc: 0.88740\n",
            "Epoch 69/250\n",
            "Epoch 00069: val_loss did not improve\n",
            "86s - loss: 0.36665 - acc: 0.87088 - val_loss: 0.34192 - val_acc: 0.88920\n",
            "Epoch 70/250\n",
            "Epoch 00070: val_loss did not improve\n",
            "87s - loss: 0.36931 - acc: 0.86980 - val_loss: 0.34652 - val_acc: 0.88720\n",
            "Epoch 71/250\n",
            "Epoch 00071: val_loss did not improve\n",
            "87s - loss: 0.36702 - acc: 0.86926 - val_loss: 0.34363 - val_acc: 0.88840\n",
            "Epoch 72/250\n",
            "Epoch 00072: val_loss improved from 0.33914 to 0.32659, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.36542 - acc: 0.87052 - val_loss: 0.32659 - val_acc: 0.89260\n",
            "Epoch 73/250\n",
            "Epoch 00073: val_loss did not improve\n",
            "87s - loss: 0.36689 - acc: 0.86930 - val_loss: 0.33566 - val_acc: 0.88720\n",
            "Epoch 74/250\n",
            "Epoch 00074: val_loss did not improve\n",
            "88s - loss: 0.36703 - acc: 0.87118 - val_loss: 0.33020 - val_acc: 0.89290\n",
            "Epoch 75/250\n",
            "Epoch 00075: val_loss did not improve\n",
            "88s - loss: 0.36615 - acc: 0.86880 - val_loss: 0.32996 - val_acc: 0.89150\n",
            "Epoch 76/250\n",
            "Epoch 00076: val_loss did not improve\n",
            "87s - loss: 0.36748 - acc: 0.87122 - val_loss: 0.32901 - val_acc: 0.89320\n",
            "Epoch 77/250\n",
            "Epoch 00077: val_loss did not improve\n",
            "87s - loss: 0.35864 - acc: 0.87392 - val_loss: 0.34450 - val_acc: 0.88570\n",
            "Epoch 78/250\n",
            "Epoch 00078: val_loss did not improve\n",
            "87s - loss: 0.35870 - acc: 0.87334 - val_loss: 0.32878 - val_acc: 0.89150\n",
            "Epoch 79/250\n",
            "Epoch 00079: val_loss improved from 0.32659 to 0.32394, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.35883 - acc: 0.87382 - val_loss: 0.32394 - val_acc: 0.89090\n",
            "Epoch 80/250\n",
            "Epoch 00080: val_loss did not improve\n",
            "87s - loss: 0.35504 - acc: 0.87554 - val_loss: 0.33877 - val_acc: 0.88810\n",
            "Epoch 81/250\n",
            "Epoch 00081: val_loss did not improve\n",
            "87s - loss: 0.35859 - acc: 0.87366 - val_loss: 0.33181 - val_acc: 0.89250\n",
            "Epoch 82/250\n",
            "Epoch 00082: val_loss did not improve\n",
            "87s - loss: 0.35599 - acc: 0.87450 - val_loss: 0.33198 - val_acc: 0.89240\n",
            "Epoch 83/250\n",
            "Epoch 00083: val_loss did not improve\n",
            "87s - loss: 0.34971 - acc: 0.87638 - val_loss: 0.33516 - val_acc: 0.88910\n",
            "Epoch 84/250\n",
            "Epoch 00084: val_loss did not improve\n",
            "87s - loss: 0.35165 - acc: 0.87682 - val_loss: 0.33521 - val_acc: 0.88850\n",
            "Epoch 85/250\n",
            "Epoch 00085: val_loss improved from 0.32394 to 0.32364, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.35221 - acc: 0.87530 - val_loss: 0.32364 - val_acc: 0.89200\n",
            "Epoch 86/250\n",
            "Epoch 00086: val_loss improved from 0.32364 to 0.32117, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.35482 - acc: 0.87546 - val_loss: 0.32117 - val_acc: 0.89250\n",
            "Epoch 87/250\n",
            "Epoch 00087: val_loss did not improve\n",
            "87s - loss: 0.34525 - acc: 0.88004 - val_loss: 0.32914 - val_acc: 0.89210\n",
            "Epoch 88/250\n",
            "Epoch 00088: val_loss did not improve\n",
            "86s - loss: 0.35054 - acc: 0.87694 - val_loss: 0.32780 - val_acc: 0.89270\n",
            "Epoch 89/250\n",
            "Epoch 00089: val_loss did not improve\n",
            "86s - loss: 0.34390 - acc: 0.87854 - val_loss: 0.33613 - val_acc: 0.89050\n",
            "Epoch 90/250\n",
            "Epoch 00090: val_loss did not improve\n",
            "87s - loss: 0.34824 - acc: 0.87608 - val_loss: 0.32306 - val_acc: 0.89060\n",
            "Epoch 91/250\n",
            "Epoch 00091: val_loss did not improve\n",
            "87s - loss: 0.34036 - acc: 0.87992 - val_loss: 0.33054 - val_acc: 0.89080\n",
            "Epoch 92/250\n",
            "Epoch 00092: val_loss improved from 0.32117 to 0.31934, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.34152 - acc: 0.87926 - val_loss: 0.31934 - val_acc: 0.89350\n",
            "Epoch 93/250\n",
            "Epoch 00093: val_loss did not improve\n",
            "87s - loss: 0.33996 - acc: 0.88098 - val_loss: 0.31982 - val_acc: 0.89460\n",
            "Epoch 94/250\n",
            "Epoch 00094: val_loss did not improve\n",
            "87s - loss: 0.34002 - acc: 0.87976 - val_loss: 0.33232 - val_acc: 0.89100\n",
            "Epoch 95/250\n",
            "Epoch 00095: val_loss did not improve\n",
            "86s - loss: 0.34099 - acc: 0.88050 - val_loss: 0.32506 - val_acc: 0.89280\n",
            "Epoch 96/250\n",
            "Epoch 00096: val_loss did not improve\n",
            "90s - loss: 0.34334 - acc: 0.87942 - val_loss: 0.32872 - val_acc: 0.89520\n",
            "Epoch 97/250\n",
            "Epoch 00097: val_loss improved from 0.31934 to 0.31393, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.33494 - acc: 0.88250 - val_loss: 0.31393 - val_acc: 0.89810\n",
            "Epoch 98/250\n",
            "Epoch 00098: val_loss did not improve\n",
            "87s - loss: 0.33133 - acc: 0.88324 - val_loss: 0.32361 - val_acc: 0.89580\n",
            "Epoch 99/250\n",
            "Epoch 00099: val_loss did not improve\n",
            "87s - loss: 0.33608 - acc: 0.88118 - val_loss: 0.32216 - val_acc: 0.89620\n",
            "Epoch 100/250\n",
            "Epoch 00100: val_loss did not improve\n",
            "87s - loss: 0.33643 - acc: 0.88196 - val_loss: 0.32543 - val_acc: 0.89500\n",
            "Epoch 101/250\n",
            "Epoch 00101: val_loss did not improve\n",
            "87s - loss: 0.33425 - acc: 0.88174 - val_loss: 0.32809 - val_acc: 0.89050\n",
            "Epoch 102/250\n",
            "Epoch 00102: val_loss did not improve\n",
            "87s - loss: 0.33794 - acc: 0.88248 - val_loss: 0.32348 - val_acc: 0.89790\n",
            "Epoch 103/250\n",
            "Epoch 00103: val_loss did not improve\n",
            "87s - loss: 0.33825 - acc: 0.88088 - val_loss: 0.32913 - val_acc: 0.89260\n",
            "Epoch 104/250\n",
            "Epoch 00104: val_loss did not improve\n",
            "87s - loss: 0.33370 - acc: 0.88330 - val_loss: 0.31891 - val_acc: 0.89840\n",
            "Epoch 105/250\n",
            "Epoch 00105: val_loss did not improve\n",
            "86s - loss: 0.33500 - acc: 0.88226 - val_loss: 0.32942 - val_acc: 0.89570\n",
            "Epoch 106/250\n",
            "Epoch 00106: val_loss did not improve\n",
            "86s - loss: 0.33075 - acc: 0.88370 - val_loss: 0.33857 - val_acc: 0.88900\n",
            "Epoch 107/250\n",
            "Epoch 00107: val_loss did not improve\n",
            "87s - loss: 0.32853 - acc: 0.88492 - val_loss: 0.32943 - val_acc: 0.89340\n",
            "Epoch 108/250\n",
            "Epoch 00108: val_loss did not improve\n",
            "87s - loss: 0.33127 - acc: 0.88350 - val_loss: 0.32362 - val_acc: 0.89730\n",
            "Epoch 109/250\n",
            "Epoch 00109: val_loss did not improve\n",
            "86s - loss: 0.32394 - acc: 0.88444 - val_loss: 0.31834 - val_acc: 0.89680\n",
            "Epoch 110/250\n",
            "Epoch 00110: val_loss did not improve\n",
            "87s - loss: 0.33046 - acc: 0.88196 - val_loss: 0.32753 - val_acc: 0.89470\n",
            "Epoch 111/250\n",
            "Epoch 00111: val_loss did not improve\n",
            "87s - loss: 0.32755 - acc: 0.88534 - val_loss: 0.32002 - val_acc: 0.89910\n",
            "Epoch 112/250\n",
            "Epoch 00112: val_loss did not improve\n",
            "87s - loss: 0.32774 - acc: 0.88398 - val_loss: 0.32458 - val_acc: 0.89670\n",
            "Epoch 113/250\n",
            "Epoch 00113: val_loss did not improve\n",
            "86s - loss: 0.32577 - acc: 0.88460 - val_loss: 0.32208 - val_acc: 0.89430\n",
            "Epoch 114/250\n",
            "Epoch 00114: val_loss did not improve\n",
            "87s - loss: 0.32780 - acc: 0.88544 - val_loss: 0.32286 - val_acc: 0.89540\n",
            "Epoch 115/250\n",
            "Epoch 00115: val_loss did not improve\n",
            "86s - loss: 0.32548 - acc: 0.88462 - val_loss: 0.32188 - val_acc: 0.89360\n",
            "Epoch 116/250\n",
            "Epoch 00116: val_loss did not improve\n",
            "85s - loss: 0.32292 - acc: 0.88472 - val_loss: 0.32504 - val_acc: 0.89580\n",
            "Epoch 117/250\n",
            "Epoch 00117: val_loss did not improve\n",
            "89s - loss: 0.31878 - acc: 0.88778 - val_loss: 0.31713 - val_acc: 0.89670\n",
            "Epoch 118/250\n",
            "Epoch 00118: val_loss did not improve\n",
            "87s - loss: 0.32452 - acc: 0.88560 - val_loss: 0.31480 - val_acc: 0.89650\n",
            "Epoch 119/250\n",
            "Epoch 00119: val_loss did not improve\n",
            "86s - loss: 0.31969 - acc: 0.88528 - val_loss: 0.32259 - val_acc: 0.89520\n",
            "Epoch 120/250\n",
            "Epoch 00120: val_loss did not improve\n",
            "85s - loss: 0.32116 - acc: 0.88794 - val_loss: 0.31923 - val_acc: 0.89530\n",
            "Epoch 121/250\n",
            "Epoch 00121: val_loss did not improve\n",
            "85s - loss: 0.31632 - acc: 0.88928 - val_loss: 0.34183 - val_acc: 0.89110\n",
            "Epoch 122/250\n",
            "Epoch 00122: val_loss did not improve\n",
            "85s - loss: 0.31942 - acc: 0.88710 - val_loss: 0.32043 - val_acc: 0.89620\n",
            "Epoch 123/250\n",
            "Epoch 00123: val_loss did not improve\n",
            "85s - loss: 0.31647 - acc: 0.88772 - val_loss: 0.32225 - val_acc: 0.89790\n",
            "Epoch 124/250\n",
            "Epoch 00124: val_loss did not improve\n",
            "85s - loss: 0.31672 - acc: 0.88712 - val_loss: 0.32353 - val_acc: 0.89590\n",
            "Epoch 125/250\n",
            "Epoch 00125: val_loss did not improve\n",
            "85s - loss: 0.31481 - acc: 0.88874 - val_loss: 0.33476 - val_acc: 0.89400\n",
            "Epoch 126/250\n",
            "Epoch 00126: val_loss did not improve\n",
            "85s - loss: 0.31933 - acc: 0.88646 - val_loss: 0.33011 - val_acc: 0.89380\n",
            "Epoch 127/250\n",
            "Epoch 00127: val_loss did not improve\n",
            "85s - loss: 0.30996 - acc: 0.88892 - val_loss: 0.32016 - val_acc: 0.89740\n",
            "Epoch 128/250\n",
            "Epoch 00128: val_loss did not improve\n",
            "85s - loss: 0.31377 - acc: 0.88954 - val_loss: 0.33408 - val_acc: 0.89390\n",
            "Epoch 129/250\n",
            "Epoch 00129: val_loss did not improve\n",
            "89s - loss: 0.32052 - acc: 0.88706 - val_loss: 0.31795 - val_acc: 0.89770\n",
            "Epoch 130/250\n",
            "Epoch 00130: val_loss did not improve\n",
            "89s - loss: 0.30924 - acc: 0.89184 - val_loss: 0.32451 - val_acc: 0.89540\n",
            "Epoch 131/250\n",
            "Epoch 00131: val_loss did not improve\n",
            "86s - loss: 0.30987 - acc: 0.89116 - val_loss: 0.33376 - val_acc: 0.89270\n",
            "Epoch 132/250\n",
            "Epoch 00132: val_loss did not improve\n",
            "86s - loss: 0.30708 - acc: 0.89166 - val_loss: 0.32596 - val_acc: 0.89550\n",
            "Epoch 133/250\n",
            "Epoch 00133: val_loss did not improve\n",
            "89s - loss: 0.31392 - acc: 0.88922 - val_loss: 0.32515 - val_acc: 0.89530\n",
            "Epoch 134/250\n",
            "Epoch 00134: val_loss improved from 0.31393 to 0.31271, saving model to bestCheckPoint.pth\n",
            "88s - loss: 0.30749 - acc: 0.89208 - val_loss: 0.31271 - val_acc: 0.89920\n",
            "Epoch 135/250\n",
            "Epoch 00135: val_loss did not improve\n",
            "88s - loss: 0.31111 - acc: 0.88974 - val_loss: 0.32697 - val_acc: 0.89690\n",
            "Epoch 136/250\n",
            "Epoch 00136: val_loss did not improve\n",
            "89s - loss: 0.31206 - acc: 0.88980 - val_loss: 0.31689 - val_acc: 0.90020\n",
            "Epoch 137/250\n",
            "Epoch 00137: val_loss did not improve\n",
            "89s - loss: 0.31174 - acc: 0.88944 - val_loss: 0.32990 - val_acc: 0.89420\n",
            "Epoch 138/250\n",
            "Epoch 00138: val_loss did not improve\n",
            "88s - loss: 0.31290 - acc: 0.88886 - val_loss: 0.32792 - val_acc: 0.89680\n",
            "Epoch 139/250\n",
            "Epoch 00139: val_loss did not improve\n",
            "89s - loss: 0.30720 - acc: 0.89088 - val_loss: 0.32068 - val_acc: 0.89690\n",
            "Epoch 140/250\n",
            "Epoch 00140: val_loss did not improve\n",
            "88s - loss: 0.30569 - acc: 0.89212 - val_loss: 0.32638 - val_acc: 0.89760\n",
            "Epoch 141/250\n",
            "Epoch 00141: val_loss did not improve\n",
            "88s - loss: 0.30279 - acc: 0.89308 - val_loss: 0.32100 - val_acc: 0.90060\n",
            "Epoch 142/250\n",
            "Epoch 00142: val_loss did not improve\n",
            "88s - loss: 0.30533 - acc: 0.89060 - val_loss: 0.31331 - val_acc: 0.89980\n",
            "Epoch 143/250\n",
            "Epoch 00143: val_loss improved from 0.31271 to 0.30958, saving model to bestCheckPoint.pth\n",
            "88s - loss: 0.30402 - acc: 0.89150 - val_loss: 0.30958 - val_acc: 0.90220\n",
            "Epoch 144/250\n",
            "Epoch 00144: val_loss did not improve\n",
            "89s - loss: 0.30402 - acc: 0.89298 - val_loss: 0.32654 - val_acc: 0.89940\n",
            "Epoch 145/250\n",
            "Epoch 00145: val_loss did not improve\n",
            "88s - loss: 0.30592 - acc: 0.89154 - val_loss: 0.31925 - val_acc: 0.89890\n",
            "Epoch 146/250\n",
            "Epoch 00146: val_loss improved from 0.30958 to 0.30793, saving model to bestCheckPoint.pth\n",
            "87s - loss: 0.30438 - acc: 0.89296 - val_loss: 0.30793 - val_acc: 0.90340\n",
            "Epoch 147/250\n",
            "Epoch 00147: val_loss did not improve\n",
            "88s - loss: 0.30388 - acc: 0.89302 - val_loss: 0.31731 - val_acc: 0.90010\n",
            "Epoch 148/250\n",
            "Epoch 00148: val_loss did not improve\n",
            "88s - loss: 0.30250 - acc: 0.89190 - val_loss: 0.31992 - val_acc: 0.89890\n",
            "Epoch 149/250\n",
            "Epoch 00149: val_loss did not improve\n",
            "88s - loss: 0.30071 - acc: 0.89220 - val_loss: 0.32411 - val_acc: 0.89690\n",
            "Epoch 150/250\n",
            "Epoch 00150: val_loss did not improve\n",
            "88s - loss: 0.29986 - acc: 0.89494 - val_loss: 0.32110 - val_acc: 0.89820\n",
            "Epoch 151/250\n",
            "Epoch 00151: val_loss did not improve\n",
            "88s - loss: 0.29924 - acc: 0.89414 - val_loss: 0.32908 - val_acc: 0.89780\n",
            "Epoch 152/250\n",
            "Epoch 00152: val_loss did not improve\n",
            "87s - loss: 0.29933 - acc: 0.89356 - val_loss: 0.32097 - val_acc: 0.89750\n",
            "Epoch 153/250\n",
            "Epoch 00153: val_loss did not improve\n",
            "89s - loss: 0.29674 - acc: 0.89586 - val_loss: 0.31909 - val_acc: 0.89860\n",
            "Epoch 154/250\n",
            "Epoch 00154: val_loss did not improve\n",
            "88s - loss: 0.29960 - acc: 0.89502 - val_loss: 0.31500 - val_acc: 0.90060\n",
            "Epoch 155/250\n",
            "Epoch 00155: val_loss did not improve\n",
            "88s - loss: 0.30411 - acc: 0.89208 - val_loss: 0.31446 - val_acc: 0.90070\n",
            "Epoch 156/250\n",
            "Epoch 00156: val_loss did not improve\n",
            "88s - loss: 0.29594 - acc: 0.89400 - val_loss: 0.32136 - val_acc: 0.89950\n",
            "Epoch 157/250\n",
            "Epoch 00157: val_loss did not improve\n",
            "87s - loss: 0.29786 - acc: 0.89492 - val_loss: 0.31367 - val_acc: 0.90120\n",
            "Epoch 158/250\n",
            "Epoch 00158: val_loss did not improve\n",
            "88s - loss: 0.29797 - acc: 0.89306 - val_loss: 0.33567 - val_acc: 0.89390\n",
            "Epoch 159/250\n",
            "Epoch 00159: val_loss did not improve\n",
            "88s - loss: 0.30063 - acc: 0.89398 - val_loss: 0.31689 - val_acc: 0.90240\n",
            "Epoch 160/250\n",
            "Epoch 00160: val_loss did not improve\n",
            "87s - loss: 0.29532 - acc: 0.89478 - val_loss: 0.31290 - val_acc: 0.90260\n",
            "Epoch 161/250\n",
            "Epoch 00161: val_loss did not improve\n",
            "87s - loss: 0.29295 - acc: 0.89528 - val_loss: 0.32138 - val_acc: 0.89810\n",
            "Epoch 162/250\n",
            "Epoch 00162: val_loss did not improve\n",
            "87s - loss: 0.29890 - acc: 0.89388 - val_loss: 0.31907 - val_acc: 0.90140\n",
            "Epoch 163/250\n",
            "Epoch 00163: val_loss did not improve\n",
            "88s - loss: 0.29498 - acc: 0.89472 - val_loss: 0.32521 - val_acc: 0.89690\n",
            "Epoch 164/250\n",
            "Epoch 00164: val_loss did not improve\n",
            "88s - loss: 0.29671 - acc: 0.89450 - val_loss: 0.31515 - val_acc: 0.90220\n",
            "Epoch 165/250\n",
            "Epoch 00165: val_loss did not improve\n",
            "88s - loss: 0.29580 - acc: 0.89478 - val_loss: 0.31948 - val_acc: 0.89800\n",
            "Epoch 166/250\n",
            "Epoch 00166: val_loss did not improve\n",
            "88s - loss: 0.29699 - acc: 0.89354 - val_loss: 0.31313 - val_acc: 0.89830\n",
            "Epoch 167/250\n",
            "Epoch 00167: val_loss did not improve\n",
            "87s - loss: 0.29166 - acc: 0.89796 - val_loss: 0.32240 - val_acc: 0.89910\n",
            "Epoch 168/250\n",
            "Epoch 00168: val_loss did not improve\n",
            "88s - loss: 0.29215 - acc: 0.89690 - val_loss: 0.31252 - val_acc: 0.90170\n",
            "Epoch 169/250\n",
            "Epoch 00169: val_loss did not improve\n",
            "88s - loss: 0.28995 - acc: 0.89608 - val_loss: 0.32411 - val_acc: 0.89860\n",
            "Epoch 170/250\n",
            "Epoch 00170: val_loss did not improve\n",
            "88s - loss: 0.29379 - acc: 0.89416 - val_loss: 0.31515 - val_acc: 0.90200\n",
            "Epoch 171/250\n",
            "Epoch 00171: val_loss did not improve\n",
            "88s - loss: 0.29399 - acc: 0.89724 - val_loss: 0.31279 - val_acc: 0.90320\n",
            "Epoch 172/250\n",
            "Epoch 00172: val_loss did not improve\n",
            "88s - loss: 0.28459 - acc: 0.89900 - val_loss: 0.32618 - val_acc: 0.90000\n",
            "Epoch 173/250\n",
            "Epoch 00173: val_loss did not improve\n",
            "87s - loss: 0.29005 - acc: 0.89802 - val_loss: 0.31859 - val_acc: 0.90140\n",
            "Epoch 174/250\n",
            "Epoch 00174: val_loss did not improve\n",
            "88s - loss: 0.28875 - acc: 0.89720 - val_loss: 0.31195 - val_acc: 0.90180\n",
            "Epoch 175/250\n",
            "Epoch 00175: val_loss did not improve\n",
            "88s - loss: 0.29059 - acc: 0.89638 - val_loss: 0.32148 - val_acc: 0.90050\n",
            "Epoch 176/250\n",
            "Epoch 00176: val_loss did not improve\n",
            "88s - loss: 0.28930 - acc: 0.89644 - val_loss: 0.31714 - val_acc: 0.90100\n",
            "Epoch 177/250\n",
            "Epoch 00177: val_loss did not improve\n",
            "88s - loss: 0.29220 - acc: 0.89542 - val_loss: 0.31117 - val_acc: 0.90180\n",
            "Epoch 178/250\n",
            "Epoch 00178: val_loss did not improve\n",
            "88s - loss: 0.28751 - acc: 0.89838 - val_loss: 0.31960 - val_acc: 0.90030\n",
            "Epoch 179/250\n",
            "Epoch 00179: val_loss did not improve\n",
            "88s - loss: 0.28957 - acc: 0.89738 - val_loss: 0.33131 - val_acc: 0.89670\n",
            "Epoch 180/250\n",
            "Epoch 00180: val_loss did not improve\n",
            "88s - loss: 0.28723 - acc: 0.89796 - val_loss: 0.31480 - val_acc: 0.89870\n",
            "Epoch 181/250\n",
            "Epoch 00181: val_loss did not improve\n",
            "88s - loss: 0.28560 - acc: 0.89878 - val_loss: 0.32230 - val_acc: 0.90210\n",
            "Epoch 182/250\n",
            "Epoch 00182: val_loss did not improve\n",
            "87s - loss: 0.28800 - acc: 0.89776 - val_loss: 0.32796 - val_acc: 0.89930\n",
            "Epoch 183/250\n",
            "Epoch 00183: val_loss did not improve\n",
            "88s - loss: 0.29062 - acc: 0.89806 - val_loss: 0.30997 - val_acc: 0.90220\n",
            "Epoch 184/250\n",
            "Epoch 00184: val_loss did not improve\n",
            "88s - loss: 0.28356 - acc: 0.89912 - val_loss: 0.33174 - val_acc: 0.89800\n",
            "Epoch 185/250\n",
            "Epoch 00185: val_loss did not improve\n",
            "89s - loss: 0.28729 - acc: 0.89592 - val_loss: 0.32371 - val_acc: 0.89760\n",
            "Epoch 186/250\n",
            "Epoch 00186: val_loss did not improve\n",
            "88s - loss: 0.28346 - acc: 0.89952 - val_loss: 0.31523 - val_acc: 0.90100\n",
            "Epoch 187/250\n",
            "Epoch 00187: val_loss did not improve\n",
            "88s - loss: 0.28921 - acc: 0.89728 - val_loss: 0.31038 - val_acc: 0.90070\n",
            "Epoch 188/250\n",
            "Epoch 00188: val_loss did not improve\n",
            "88s - loss: 0.28905 - acc: 0.89680 - val_loss: 0.33009 - val_acc: 0.89560\n",
            "Epoch 189/250\n",
            "Epoch 00189: val_loss did not improve\n",
            "87s - loss: 0.28510 - acc: 0.90000 - val_loss: 0.32127 - val_acc: 0.89710\n",
            "Epoch 190/250\n",
            "Epoch 00190: val_loss did not improve\n",
            "88s - loss: 0.28589 - acc: 0.89776 - val_loss: 0.31468 - val_acc: 0.90020\n",
            "Epoch 191/250\n",
            "Epoch 00191: val_loss did not improve\n",
            "88s - loss: 0.28259 - acc: 0.89866 - val_loss: 0.31318 - val_acc: 0.90200\n",
            "Epoch 192/250\n",
            "Epoch 00192: val_loss did not improve\n",
            "88s - loss: 0.28605 - acc: 0.89870 - val_loss: 0.32357 - val_acc: 0.90130\n",
            "Epoch 193/250\n",
            "Epoch 00193: val_loss did not improve\n",
            "88s - loss: 0.28238 - acc: 0.89884 - val_loss: 0.32066 - val_acc: 0.90110\n",
            "Epoch 194/250\n",
            "Epoch 00194: val_loss did not improve\n",
            "88s - loss: 0.28720 - acc: 0.89718 - val_loss: 0.31614 - val_acc: 0.89980\n",
            "Epoch 195/250\n",
            "Epoch 00195: val_loss did not improve\n",
            "88s - loss: 0.28455 - acc: 0.89846 - val_loss: 0.31671 - val_acc: 0.89990\n",
            "Epoch 196/250\n",
            "Epoch 00196: val_loss did not improve\n",
            "88s - loss: 0.28406 - acc: 0.89778 - val_loss: 0.31759 - val_acc: 0.89970\n",
            "Epoch 197/250\n",
            "Epoch 00197: val_loss did not improve\n",
            "88s - loss: 0.27871 - acc: 0.90260 - val_loss: 0.31929 - val_acc: 0.90280\n",
            "Epoch 198/250\n",
            "Epoch 00198: val_loss did not improve\n",
            "88s - loss: 0.28406 - acc: 0.89996 - val_loss: 0.32900 - val_acc: 0.89660\n",
            "Epoch 199/250\n",
            "Epoch 00199: val_loss did not improve\n",
            "88s - loss: 0.28109 - acc: 0.89962 - val_loss: 0.31673 - val_acc: 0.90030\n",
            "Epoch 200/250\n",
            "Epoch 00200: val_loss did not improve\n",
            "88s - loss: 0.28127 - acc: 0.89896 - val_loss: 0.31944 - val_acc: 0.90000\n",
            "Epoch 201/250\n",
            "Epoch 00201: val_loss did not improve\n",
            "87s - loss: 0.28280 - acc: 0.89882 - val_loss: 0.31803 - val_acc: 0.90240\n",
            "Epoch 202/250\n",
            "Epoch 00202: val_loss did not improve\n",
            "88s - loss: 0.28084 - acc: 0.89964 - val_loss: 0.32564 - val_acc: 0.90110\n",
            "Epoch 203/250\n",
            "Epoch 00203: val_loss did not improve\n",
            "87s - loss: 0.27941 - acc: 0.89908 - val_loss: 0.32502 - val_acc: 0.90040\n",
            "Epoch 204/250\n",
            "Epoch 00204: val_loss did not improve\n",
            "88s - loss: 0.27844 - acc: 0.90050 - val_loss: 0.31278 - val_acc: 0.90160\n",
            "Epoch 205/250\n",
            "Epoch 00205: val_loss did not improve\n",
            "88s - loss: 0.27627 - acc: 0.90232 - val_loss: 0.32333 - val_acc: 0.90010\n",
            "Epoch 206/250\n",
            "Epoch 00206: val_loss did not improve\n",
            "88s - loss: 0.27547 - acc: 0.90254 - val_loss: 0.32479 - val_acc: 0.89870\n",
            "Epoch 207/250\n",
            "Epoch 00207: val_loss did not improve\n",
            "88s - loss: 0.27678 - acc: 0.90236 - val_loss: 0.31534 - val_acc: 0.90340\n",
            "Epoch 208/250\n",
            "Epoch 00208: val_loss did not improve\n",
            "88s - loss: 0.27879 - acc: 0.90156 - val_loss: 0.31303 - val_acc: 0.90470\n",
            "Epoch 209/250\n",
            "Epoch 00209: val_loss did not improve\n",
            "87s - loss: 0.28043 - acc: 0.90248 - val_loss: 0.31967 - val_acc: 0.90000\n",
            "Epoch 210/250\n",
            "Epoch 00210: val_loss did not improve\n",
            "87s - loss: 0.27283 - acc: 0.90228 - val_loss: 0.31017 - val_acc: 0.90450\n",
            "Epoch 211/250\n",
            "Epoch 00211: val_loss did not improve\n",
            "88s - loss: 0.27695 - acc: 0.90048 - val_loss: 0.32481 - val_acc: 0.90120\n",
            "Epoch 212/250\n",
            "Epoch 00212: val_loss did not improve\n",
            "87s - loss: 0.27828 - acc: 0.90262 - val_loss: 0.31297 - val_acc: 0.90360\n",
            "Epoch 213/250\n",
            "Epoch 00213: val_loss did not improve\n",
            "88s - loss: 0.27627 - acc: 0.90092 - val_loss: 0.31113 - val_acc: 0.90340\n",
            "Epoch 214/250\n",
            "Epoch 00214: val_loss did not improve\n",
            "87s - loss: 0.27593 - acc: 0.90302 - val_loss: 0.31652 - val_acc: 0.90310\n",
            "Epoch 215/250\n",
            "Epoch 00215: val_loss did not improve\n",
            "87s - loss: 0.27935 - acc: 0.90172 - val_loss: 0.31926 - val_acc: 0.90310\n",
            "Epoch 216/250\n",
            "Epoch 00216: val_loss did not improve\n",
            "87s - loss: 0.27803 - acc: 0.90190 - val_loss: 0.31840 - val_acc: 0.90300\n",
            "Epoch 217/250\n",
            "Epoch 00217: val_loss did not improve\n",
            "87s - loss: 0.27143 - acc: 0.90224 - val_loss: 0.31917 - val_acc: 0.90300\n",
            "Epoch 218/250\n",
            "Epoch 00218: val_loss did not improve\n",
            "87s - loss: 0.27462 - acc: 0.90306 - val_loss: 0.32434 - val_acc: 0.90080\n",
            "Epoch 219/250\n",
            "Epoch 00219: val_loss did not improve\n",
            "87s - loss: 0.27115 - acc: 0.90286 - val_loss: 0.31297 - val_acc: 0.90280\n",
            "Epoch 220/250\n",
            "Epoch 00220: val_loss did not improve\n",
            "87s - loss: 0.27096 - acc: 0.90384 - val_loss: 0.31807 - val_acc: 0.90210\n",
            "Epoch 221/250\n",
            "Epoch 00221: val_loss did not improve\n",
            "88s - loss: 0.26986 - acc: 0.90472 - val_loss: 0.31959 - val_acc: 0.90240\n",
            "Epoch 222/250\n",
            "Epoch 00222: val_loss did not improve\n",
            "87s - loss: 0.27457 - acc: 0.90146 - val_loss: 0.31981 - val_acc: 0.90190\n",
            "Epoch 223/250\n",
            "Epoch 00223: val_loss did not improve\n",
            "87s - loss: 0.27054 - acc: 0.90274 - val_loss: 0.32668 - val_acc: 0.90170\n",
            "Epoch 224/250\n",
            "Epoch 00224: val_loss did not improve\n",
            "88s - loss: 0.27264 - acc: 0.90384 - val_loss: 0.31780 - val_acc: 0.90350\n",
            "Epoch 225/250\n",
            "Epoch 00225: val_loss did not improve\n",
            "87s - loss: 0.27086 - acc: 0.90326 - val_loss: 0.32098 - val_acc: 0.90310\n",
            "Epoch 226/250\n",
            "Epoch 00226: val_loss did not improve\n",
            "87s - loss: 0.27080 - acc: 0.90420 - val_loss: 0.31435 - val_acc: 0.90520\n",
            "Epoch 227/250\n",
            "Epoch 00227: val_loss did not improve\n",
            "87s - loss: 0.27267 - acc: 0.90258 - val_loss: 0.31900 - val_acc: 0.90390\n",
            "Epoch 228/250\n",
            "Epoch 00228: val_loss did not improve\n",
            "87s - loss: 0.26953 - acc: 0.90360 - val_loss: 0.31780 - val_acc: 0.90100\n",
            "Epoch 229/250\n",
            "Epoch 00229: val_loss did not improve\n",
            "89s - loss: 0.27313 - acc: 0.90386 - val_loss: 0.31246 - val_acc: 0.90460\n",
            "Epoch 230/250\n",
            "Epoch 00230: val_loss did not improve\n",
            "88s - loss: 0.27086 - acc: 0.90370 - val_loss: 0.32092 - val_acc: 0.90380\n",
            "Epoch 231/250\n",
            "Epoch 00231: val_loss did not improve\n",
            "89s - loss: 0.26895 - acc: 0.90576 - val_loss: 0.31334 - val_acc: 0.90350\n",
            "Epoch 232/250\n",
            "Epoch 00232: val_loss did not improve\n",
            "89s - loss: 0.26923 - acc: 0.90444 - val_loss: 0.32522 - val_acc: 0.90100\n",
            "Epoch 233/250\n",
            "Epoch 00233: val_loss did not improve\n",
            "88s - loss: 0.27140 - acc: 0.90392 - val_loss: 0.31769 - val_acc: 0.90250\n",
            "Epoch 234/250\n",
            "Epoch 00234: val_loss did not improve\n",
            "87s - loss: 0.27082 - acc: 0.90220 - val_loss: 0.31915 - val_acc: 0.90270\n",
            "Epoch 235/250\n",
            "Epoch 00235: val_loss did not improve\n",
            "86s - loss: 0.26611 - acc: 0.90540 - val_loss: 0.32568 - val_acc: 0.90160\n",
            "Epoch 236/250\n",
            "Epoch 00236: val_loss did not improve\n",
            "85s - loss: 0.26268 - acc: 0.90514 - val_loss: 0.32789 - val_acc: 0.90250\n",
            "Epoch 237/250\n",
            "Epoch 00237: val_loss did not improve\n",
            "86s - loss: 0.26816 - acc: 0.90466 - val_loss: 0.32307 - val_acc: 0.90250\n",
            "Epoch 238/250\n",
            "Epoch 00238: val_loss did not improve\n",
            "86s - loss: 0.26860 - acc: 0.90574 - val_loss: 0.30967 - val_acc: 0.90500\n",
            "Epoch 239/250\n",
            "Epoch 00239: val_loss did not improve\n",
            "86s - loss: 0.26375 - acc: 0.90580 - val_loss: 0.31122 - val_acc: 0.90540\n",
            "Epoch 240/250\n",
            "Epoch 00240: val_loss did not improve\n",
            "87s - loss: 0.26929 - acc: 0.90484 - val_loss: 0.32539 - val_acc: 0.89970\n",
            "Epoch 241/250\n",
            "Epoch 00241: val_loss did not improve\n",
            "87s - loss: 0.26353 - acc: 0.90470 - val_loss: 0.31992 - val_acc: 0.90260\n",
            "Epoch 242/250\n",
            "Epoch 00242: val_loss did not improve\n",
            "87s - loss: 0.26930 - acc: 0.90322 - val_loss: 0.31638 - val_acc: 0.90310\n",
            "Epoch 243/250\n",
            "Epoch 00243: val_loss did not improve\n",
            "87s - loss: 0.26643 - acc: 0.90538 - val_loss: 0.32168 - val_acc: 0.90430\n",
            "Epoch 244/250\n",
            "Epoch 00244: val_loss did not improve\n",
            "87s - loss: 0.26532 - acc: 0.90602 - val_loss: 0.32229 - val_acc: 0.90130\n",
            "Epoch 245/250\n",
            "Epoch 00245: val_loss did not improve\n",
            "87s - loss: 0.26879 - acc: 0.90340 - val_loss: 0.31689 - val_acc: 0.90140\n",
            "Epoch 246/250\n",
            "Epoch 00246: val_loss did not improve\n",
            "87s - loss: 0.26562 - acc: 0.90578 - val_loss: 0.31397 - val_acc: 0.90470\n",
            "Epoch 247/250\n",
            "Epoch 00247: val_loss did not improve\n",
            "87s - loss: 0.26577 - acc: 0.90582 - val_loss: 0.31744 - val_acc: 0.90390\n",
            "Epoch 248/250\n",
            "Epoch 00248: val_loss did not improve\n",
            "87s - loss: 0.26572 - acc: 0.90580 - val_loss: 0.32151 - val_acc: 0.90020\n",
            "Epoch 249/250\n",
            "Epoch 00249: val_loss did not improve\n",
            "87s - loss: 0.26495 - acc: 0.90496 - val_loss: 0.31591 - val_acc: 0.90220\n"
          ]
        }
      ],
      "source": [
        "# 모델 학습하기\n",
        "import time\n",
        "\n",
        "# train, test 데이터셋의 로더\n",
        "# 배치사이즈와 불러오는 데이터를 섞을지 결정할 수 있다\n",
        "trainloader = torch.utils.data.DataLoader(trainSet, batch_size=batch_size,shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testSet, batch_size=1,shuffle=False)\n",
        "\n",
        "# 파라미터 설정\n",
        "total_epoch = 250\n",
        "best_loss = 100 # loss를 기준으로 best_checkpoint를 저장하기 위해 100으로 설정하였음.\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005,weight_decay=1e-6) # Adam을 최적화 함수로 이용함. 파라미터는 documentation을 참조!\n",
        "loss = torch.nn.CrossEntropyLoss().cuda() # 분류문제이므로 CrossEntropyLoss를 이용함\n",
        "\n",
        "# 모델 시각화를 위해 정확도를 저장할 리스트 생성\n",
        "train_accuracys=[]\n",
        "eval_accuracys=[]\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "    start = time.time()\n",
        "    print(f'Epoch {epoch}/{total_epoch}')\n",
        "\n",
        "    # train\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    for x, target in trainloader: # 한번에 배치사이즈만큼 데이터를 불러와 모델을 학습함\n",
        "\n",
        "        optimizer.zero_grad() # 이전 loss를 누적하지 않기 위해 0으로 설정해주는 과정\n",
        "        y_pred = model(x.cuda()) # 모델의 출력값\n",
        "        cost = loss(y_pred, target.cuda()) # loss 함수를 이용하여 오차를 계산함\n",
        "    \n",
        "        cost.backward() # gradient 구하기 \n",
        "        optimizer.step() # 모델 학습\n",
        "        train_loss += cost.item()\n",
        "        \n",
        "        pred = y_pred.data.max(1, keepdim=True)[1] # 각 클래스의 확률 값 중 가장 큰 값을 가지는 클래스의 인덱스를 pred 변수로 받음\n",
        "        correct += pred.cpu().eq(target.data.view_as(pred)).sum() # pred와 target을 비교하여 맞은 개수를 구하는 과정.\n",
        "                                                                  # view_as함수는 들어가는 인수의 모양으로 맞춰주고, .eq()를 통해 pred와 target의 값이 동일한지 판단하여 True 개수 구하기\n",
        "        \n",
        "    train_loss /= len(trainloader)\n",
        "    train_accuracy = correct / len(trainloader.dataset)\n",
        "    train_accuracys.append(train_accuracy) # 그래프로 표현하기 위해 리스트에 담음\n",
        "\n",
        "    \n",
        "    # Evaluate\n",
        "    eval_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad(): # 학습하지 않기 위해\n",
        "        model.eval() # 평가 모드로 변경\n",
        "        for x, target in testloader:\n",
        "            y_pred = model(x.cuda()) # 모델의 출력값\n",
        "            cost = loss(y_pred,target.cuda()) # loss 함수를 이용하여 test 데이터의 오차를 계산함\n",
        "            eval_loss += cost\n",
        "            \n",
        "            pred = y_pred.data.max(1, keepdim=True)[1] # 각 클래스의 확률 값 중 가장 큰 값을 가지는 클래스의 인덱스를 pred 변수로 받음\n",
        "            correct += pred.cpu().eq(target.data.view_as(pred)).cpu().sum() # pred와 target을 비교하여 맞은 개수를 구하는 과정\n",
        "            \n",
        "        eval_loss /= len(testloader)\n",
        "        eval_accuracy = correct / len(testloader.dataset)\n",
        "        eval_accuracys.append(eval_accuracy) # 그래프로 표현하기 위해 리스트에 담음\n",
        "        \n",
        "        # test 데이터의 loss를 기준으로 이전 loss 보다 작을 경우 체크포인트 저장\n",
        "        if eval_loss < best_loss:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model': model,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': cost.item,\n",
        "                }, './bestCheckPiont.pth')\n",
        "            \n",
        "            print(f'Epoch {epoch:05d}: val_loss improved from {best_loss:.5f} to {eval_loss:.5f}, saving model to bestCheckPoint.pth')\n",
        "            best_loss = eval_loss\n",
        "        else:\n",
        "            print(f'Epoch {epoch:05d}: val_loss did not improve')\n",
        "        model.train()\n",
        "        \n",
        "    print(f'{int(time.time() - start)}s - loss: {train_loss:.5f} - acc: {train_accuracy:.5f} - val_loss: {eval_loss:.5f} - val_acc: {eval_accuracy:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aa03901",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-14T07:15:14.805680Z",
          "iopub.status.busy": "2023-03-14T07:15:14.804785Z",
          "iopub.status.idle": "2023-03-14T07:15:14.845858Z",
          "shell.execute_reply": "2023-03-14T07:15:14.844539Z"
        },
        "papermill": {
          "duration": 0.061851,
          "end_time": "2023-03-14T07:15:14.848404",
          "exception": false,
          "start_time": "2023-03-14T07:15:14.786553",
          "status": "completed"
        },
        "tags": [],
        "id": "3aa03901",
        "outputId": "02867563-3f8f-47a6-9124-e9cb732613bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 저장되어있는 best 체크포인트 load하기\n",
        "best_model = torch.load('./bestCheckPiont.pth')['model'] # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
        "best_model.load_state_dict(torch.load('./bestCheckPiont.pth')['model_state_dict']) # state_dict를 불러 온 후, 모델에 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e38ac581",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-14T07:15:14.885042Z",
          "iopub.status.busy": "2023-03-14T07:15:14.884212Z",
          "iopub.status.idle": "2023-03-14T07:15:36.605694Z",
          "shell.execute_reply": "2023-03-14T07:15:36.604041Z"
        },
        "papermill": {
          "duration": 21.742943,
          "end_time": "2023-03-14T07:15:36.608527",
          "exception": false,
          "start_time": "2023-03-14T07:15:14.865584",
          "status": "completed"
        },
        "tags": [],
        "id": "e38ac581",
        "outputId": "f6124da3-e0fb-4090-ecc5-8521a100d67c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 90 %\n"
          ]
        }
      ],
      "source": [
        "# prediction\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad(): # 학습하지 않기 위해\n",
        "    best_model.eval()\n",
        "    for data, label in testloader:\n",
        "        outputs = best_model(data.cuda())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += label.size(0)\n",
        "        correct += (predicted == label.cuda()).sum().item()\n",
        "        \n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe9c3e3d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-14T07:15:36.646354Z",
          "iopub.status.busy": "2023-03-14T07:15:36.645581Z",
          "iopub.status.idle": "2023-03-14T07:15:36.865348Z",
          "shell.execute_reply": "2023-03-14T07:15:36.864266Z"
        },
        "papermill": {
          "duration": 0.240057,
          "end_time": "2023-03-14T07:15:36.867340",
          "exception": false,
          "start_time": "2023-03-14T07:15:36.627283",
          "status": "completed"
        },
        "tags": [],
        "id": "fe9c3e3d",
        "outputId": "d9fb8434-4f69-4a1f-8350-1444ad782e25"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# train, test 데이터에 대한 epoch당 정확도 시각화하기 \n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(train_accuracys, label='train')\n",
        "plt.plot(eval_accuracys, label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 22006.555902,
      "end_time": "2023-03-14T07:15:38.586297",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-03-14T01:08:52.030395",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}